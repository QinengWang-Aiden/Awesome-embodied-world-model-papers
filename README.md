# Awesome-embodied-world-models-papers
A paper list that includes world models or generative video models for embodied agents.
The papers with **real robot experiments** are marked with ðŸ¤–. The papers with **open-sourced code** are marked with ðŸŒŸ.

## Survey Papers
+ [arXiv 2024.11] **Understanding World or Predicting Future? A Comprehensive Survey of World Models** [[paper](https://arxiv.org/pdf/2411.14499)]

+ [arXiv 2024.07] **Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI** [[paper](https://arxiv.org/abs/2407.06886)] [[repo](https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List)]

+ [arXiv 2024.05] **Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond** [[paper](https://arxiv.org/abs/2405.03520)] [[repo](https://github.com/GigaAI-research/General-World-Models-Survey)]

## Method Papers
+ [blog 2024.12] **Genie 2: A large-scale foundation world model** [[blog](https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/)]

+ [arXiv 2024.12] **Playable Game Generation** [[paper](https://arxiv.org/pdf/2412.00887)]

+ [arXiv 2024.10] **EVA: An Embodied World Model for Future Video Anticipation** [[paper](https://arxiv.org/abs/2410.15461)]

+ ðŸŒŸ[arXiv 2024.10] **AVID: Adapting Video Diffusion Models to World Models** [[paper](https://arxiv.org/abs/2410.12822)] [[website](https://sites.google.com/view/avid-world-model-adapters/home)] [[code](https://github.com/microsoft/causica/tree/main/research_experiments/avid)] 

+ ðŸŒŸ[blog 2024.10] **Oasis: A Universe in a Transformer** [[blog](https://decart.ai/articles/oasis-interactive-ai-video-game-model)] [[website](https://oasis-model.github.io/)] [[code](https://github.com/etched-ai/open-oasis)]

+ ðŸŒŸ[arXiv 2024.06] **Pandora: Towards General World Model with Natural Language Actions and Video States** [[paper](https://arxiv.org/abs/2406.09455)] [[website](https://world-model.maitrix.org/)] [[code](https://github.com/maitrix-org/Pandora)]

+ ðŸŒŸ[arXiv 2024.05] **Diffusion for World Modeling: Visual Details Matter in Atari** [[paper](https://arxiv.org/abs/2405.12399)]  [[website](https://diamond-wm.github.io/)] [[code](https://github.com/eloialonso/diamond)] `NeurIPS 2024`

+ ðŸŒŸ[arXiv 2024.04] **RoboDreamer: Learning Compositional World Models for Robot Imagination** [[paper](https://arxiv.org/abs/2404.12377)] [[website](https://robovideo.github.io/)] [[code](https://github.com/rainbow979/robodreamer)] `ICML 2024`

+ ðŸŒŸ[arXiv 2024.03] **3D-VLA: A 3D Vision-Language-Action Generative World Model** [[paper](https://arxiv.org/abs/2403.09631)] [[website](https://vis-www.cs.umass.edu/3dvla/)] [[code](https://github.com/UMass-Foundation-Model/3D-VLA)] `ICML 2024`

+ [arXiv 2024.02] **Genie: Generative Interactive Environments** [[paper](https://arxiv.org/abs/2402.15391)] [[website](https://sites.google.com/view/genie-2024/?pli=1)] `ICML 2024 Best Paper`

+ ðŸ¤–[arXiv 2023.10] **UniSim: Learning Interactive Real-World Simulators** [[paper](https://arxiv.org/abs/2310.06114)] [[website](https://universal-simulator.github.io/unisim/)] `ICLR 2024`

+ [arXiv 2023.02] **UniPi: Learning Universal Policies via Text-Guided Video Generation** [[paper](https://arxiv.org/pdf/2302.00111)] [[website](https://universal-policy.github.io/unipi/)] `NeurIPS 2023`

